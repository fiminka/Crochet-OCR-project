{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: all of the code still needs improvments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as levenshtein_distance\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "from yt_dlp import YoutubeDL\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import spacy\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spacy():\n",
    "    \"\"\"Load the spaCy Polish and English models.\"\"\"\n",
    "    nlp_pl = spacy.load(\"pl_core_news_sm\")  # Polish\n",
    "    nlp_en = spacy.load(\"en_core_web_md\")   # English\n",
    "    return nlp_pl, nlp_en\n",
    "\n",
    "def is_valid_word(word, nlp_pl, nlp_en, crochet_terms, short_words):\n",
    "    \"\"\"Check if a word is a valid Polish/English word or a crochet-related term.\"\"\"\n",
    "    word = word.strip()\n",
    "    \n",
    "    # Allow crochet terms\n",
    "    if word in crochet_terms:\n",
    "        return True  \n",
    "\n",
    "    # Allow CH{number} pattern\n",
    "    if re.fullmatch(r'CH\\d+', word, re.IGNORECASE):\n",
    "        return True  \n",
    "\n",
    "    # Reject if word contains numbers or special characters\n",
    "    if not re.fullmatch(r'[a-zA-ZƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈ºƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª]+', word):\n",
    "        return False  \n",
    "\n",
    "    # Allow common short words like \"in\", \"on\", \"to\", etc.\n",
    "    if len(word) < 3 and word.lower() not in short_words:\n",
    "        return False\n",
    "\n",
    "    # Check in Polish NLP\n",
    "    doc_pl = nlp_pl(word)\n",
    "    is_polish = any(token.is_alpha for token in doc_pl)\n",
    "\n",
    "    # Check in English NLP\n",
    "    doc_en = nlp_en(word)\n",
    "    is_english = any(token.has_vector and not token.is_oov for token in doc_en)\n",
    "\n",
    "    return is_polish or is_english\n",
    "\n",
    "def test_lines():\n",
    "    \"\"\"Sample lines to test.\"\"\"\n",
    "    test_lines = [\n",
    "        'LALard 2882',\n",
    "        '2252',\n",
    "        'eaby',\n",
    "        'ara',\n",
    "        'an',\n",
    "        'Piudse ULrards 2ore',\n",
    "        'tale Vao foot Zor un o7uo dol',\n",
    "        'Cleg rord un erecon y',\n",
    "        '~tner borg Mo7d7 Zox 32 dOl',\n",
    "        'Xou',\n",
    "        '8 Plla R0} Hll u@',\n",
    "        'Puenn4 Ui Vnk Ml7uioi Zo} 70 dol'\n",
    "        '2 dec, 1 sc'\n",
    "    ]\n",
    "\n",
    "    # Crochet-related terms\n",
    "    crochet_terms = {\"chain\", \"single crochet\", \"double crochet\", \"slip stitch\", \"sl st\", \"crochet\",\n",
    "                     \"hook\", \"yarn\", \"needle\", \"scissors\", \"polyfill\", \"stitch\", \"markers\", \"pattern\", \"round\", \n",
    "                     \"repeat\", \"increase\", \"decrease\", \"magic ring\", \"magic circle\", \"loop\", \"tail\", \"beginning\",\n",
    "                     \"back loop\", \"front loop\", \"back post\", \"front post\", \"except\", \"attach\", \"mark\", \"dluzsza\", \n",
    "                     \"krotsza\"}\n",
    "\n",
    "    # List of common short words to allow (e.g., \"in\", \"on\", \"to\", etc.)\n",
    "    short_words = {\"in\", \"on\", \"to\", \"a\", \"an\", \"of\", \"for\", \"by\", \"and\", \"or\", \"the\"}\n",
    "\n",
    "    # Load NLP models once here\n",
    "    nlp_pl, nlp_en = load_spacy()\n",
    "\n",
    "    valid_lines = []\n",
    "\n",
    "    for line in test_lines:\n",
    "        # Split the line into words\n",
    "        words = re.findall(r'\\b\\w+\\b', line)\n",
    "        \n",
    "        # Collect valid words in the line\n",
    "        valid_words = [word for word in words if is_valid_word(word, nlp_pl, nlp_en, crochet_terms, short_words)]\n",
    "        \n",
    "        if valid_words:\n",
    "            valid_lines.append(line)\n",
    "            print(f\"Valid words in line: '{line}' -> {valid_words}\")\n",
    "\n",
    "    return valid_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered lines and valid words:\n",
      "Valid words in line: 'LALard 2882' -> ['LALard']\n",
      "Valid words in line: 'eaby' -> ['eaby']\n",
      "Valid words in line: 'ara' -> ['ara']\n",
      "Valid words in line: 'an' -> ['an']\n",
      "Valid words in line: 'Piudse ULrards 2ore' -> ['Piudse', 'ULrards']\n",
      "Valid words in line: 'tale Vao foot Zor un o7uo dol' -> ['tale', 'Vao', 'foot', 'Zor', 'dol']\n",
      "Valid words in line: 'Cleg rord un erecon y' -> ['Cleg', 'rord', 'erecon']\n",
      "Valid words in line: '~tner borg Mo7d7 Zox 32 dOl' -> ['tner', 'borg', 'Zox', 'dOl']\n",
      "Valid words in line: 'Xou' -> ['Xou']\n",
      "Valid words in line: '8 Plla R0} Hll u@' -> ['Plla', 'Hll']\n",
      "Valid words in line: 'Puenn4 Ui Vnk Ml7uioi Zo} 70 dol2 dec, 1 sc' -> ['Vnk', 'dec']\n",
      "\n",
      "Lines that will be saved:\n",
      "['LALard 2882', 'eaby', 'ara', 'an', 'Piudse ULrards 2ore', 'tale Vao foot Zor un o7uo dol', 'Cleg rord un erecon y', '~tner borg Mo7d7 Zox 32 dOl', 'Xou', '8 Plla R0} Hll u@', 'Puenn4 Ui Vnk Ml7uioi Zo} 70 dol2 dec, 1 sc']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Filtered lines and valid words:\")\n",
    "valid_lines = test_lines()\n",
    "print(\"\\nLines that will be saved:\")\n",
    "print(valid_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levensthein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "import numpy as np\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"Reads a text file and extracts lines.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = [line.strip() for line in file if line.strip()]\n",
    "    return lines\n",
    "\n",
    "def extract_text(lines):\n",
    "    \"\"\"Extracts only the text part from 'filename, extracted_text' format.\"\"\"\n",
    "    extracted_texts = []\n",
    "    for line in lines:\n",
    "        parts = line.split(\",\", 1)  # split only at the first comma\n",
    "        if len(parts) > 1:\n",
    "            extracted_texts.append(parts[1].strip())  # keep only the text part\n",
    "    return extracted_texts\n",
    "\n",
    "def group_consecutive_similar_texts(texts, threshold=5):\n",
    "    \"\"\"\n",
    "    Groups consecutive similar texts based on Levenshtein distance.\n",
    "    \"\"\"\n",
    "    grouped_texts = []\n",
    "    temp_group = [texts[0]]  \n",
    "\n",
    "    for i in range(1, len(texts)):\n",
    "        if levenshtein_distance(texts[i], texts[i - 1]) <= threshold:\n",
    "            temp_group.append(texts[i])\n",
    "        else:\n",
    "            grouped_texts.append(temp_group)\n",
    "            temp_group = [texts[i]]\n",
    "\n",
    "    grouped_texts.append(temp_group)\n",
    "    return grouped_texts\n",
    "\n",
    "def detect_outliers_in_group(group):\n",
    "    \"\"\"\n",
    "    Identifies the most odd/outlier lines in a group using Levenshtein distance.\n",
    "    - If group has <= 4 lines, returns an empty list (skips filtering).\n",
    "    - For larger groups, calculates the average distance of each line to others.\n",
    "    - Returns the list of lines qualified for deletion.\n",
    "    \"\"\"\n",
    "    if len(group) <= 4:\n",
    "        return [] \n",
    "\n",
    "    distances = np.zeros((len(group), len(group)))\n",
    "\n",
    "    for i in range(len(group)):\n",
    "        for j in range(len(group)):\n",
    "            if i != j:\n",
    "                distances[i][j] = Levenshtein.distance(group[i], group[j])\n",
    "\n",
    "    avg_distances = np.mean(distances, axis=1)\n",
    "\n",
    "    mean_dist = np.mean(avg_distances)\n",
    "    std_dev = np.std(avg_distances)\n",
    "    threshold = mean_dist + std_dev\n",
    "\n",
    "    # Identify outliers\n",
    "    outliers = [group[i] for i in range(len(group)) if avg_distances[i] > threshold]\n",
    "\n",
    "    return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"\"\n",
    "\n",
    "lines = read_file(file_path)\n",
    "texts = extract_text(lines)\n",
    "\n",
    "similar_text_groups = group_consecutive_similar_texts(texts)\n",
    "\n",
    "for i, group in enumerate(similar_text_groups, 1):\n",
    "    outliers = detect_outliers_in_group(group)\n",
    "\n",
    "    print(f\"\\nüîπ Pattern Section {i}:\")\n",
    "    for item in group:\n",
    "        status = \"‚ùå (OUTLIER)\" if item in outliers else \"‚úÖ\"\n",
    "        print(f\"  {status} - {item}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_r_number(line):\n",
    "    match = re.match(r\"^(r\\d+(?:-\\d+)?)[;:]\", line)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def find_pattern_start(groups):\n",
    "    for i, group in enumerate(groups):\n",
    "        for line in group:\n",
    "            if re.match(r\"^(r1|row1)[;:]\", line, re.IGNORECASE):\n",
    "                return max(0, i - 1)  # Keep one group before the pattern start\n",
    "    return None\n",
    "\n",
    "def group_similar_lines(lines, threshold=3):\n",
    "    groups = []\n",
    "    current_group = []\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        r_number = extract_r_number(line)\n",
    "        \n",
    "        # Start a new group if it's a new r_number or if the current group is empty\n",
    "        if current_group:\n",
    "            prev_r_number = extract_r_number(current_group[-1])\n",
    "            if r_number and r_number != prev_r_number:\n",
    "                groups.append(current_group)\n",
    "                current_group = []\n",
    "        \n",
    "        current_group.append(line)\n",
    "        \n",
    "        if len(current_group) > 1:\n",
    "            prev_line = current_group[-2]\n",
    "            if levenshtein_distance(prev_line, line) > threshold:\n",
    "                groups.append(current_group[:-1])\n",
    "                current_group = [line]\n",
    "    \n",
    "    if current_group:\n",
    "        groups.append(current_group)\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def detect_outliers(group, threshold=3):\n",
    "    if len(group) < 2:\n",
    "        return []  # No outliers in single-line groups\n",
    "    \n",
    "    outliers = []\n",
    "    for i, line in enumerate(group):\n",
    "        for j, other in enumerate(group):\n",
    "            if i != j and levenshtein_distance(line, other) > threshold:\n",
    "                outliers.append(line)\n",
    "                break  \n",
    "    \n",
    "    return outliers\n",
    "\n",
    "def filter_and_simplify_groups(groups, threshold=3):\n",
    "    filtered_groups = []\n",
    "    \n",
    "    for group in groups:\n",
    "        if len(group) > 2:\n",
    "            outliers = detect_outliers(group, threshold)\n",
    "            filtered_group = [line for line in group if line not in outliers]\n",
    "            if len(filtered_group) > 2:\n",
    "                filtered_group = filtered_group[1:-1]  # Remove first and last occurrence\n",
    "            if filtered_group:\n",
    "                filtered_groups.append(filtered_group[0])  # Keep only one representative line per group\n",
    "        elif group:\n",
    "            filtered_groups.append(group[0])\n",
    "    \n",
    "    return filtered_groups\n",
    "\n",
    "def process_txt_file(input_filename, output_filename, threshold=3):\n",
    "    with open(input_filename, 'r', encoding='utf-8') as file:\n",
    "        lines = [line.strip() for line in file.readlines() if line.strip()]\n",
    "    \n",
    "    groups = group_similar_lines(lines, threshold)\n",
    "    pattern_start = find_pattern_start(groups)\n",
    "    \n",
    "    if pattern_start is not None:\n",
    "        groups = groups[pattern_start:]\n",
    "    \n",
    "    filtered_groups = filter_and_simplify_groups(groups, threshold)\n",
    "    \n",
    "    # Write filtered output to a new file\n",
    "    with open(output_filename, 'w', encoding='utf-8') as output_file:\n",
    "        for line in filtered_groups:\n",
    "            output_file.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_txt_file(\"data/fox_f.txt\", \"data/patterns/fox_pattern.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_pdf_with_thumbnail_and_title(txt_filename, pdf_filename, video_url):\n",
    "    \"\"\"Extract video info (thumbnail & title)\"\"\"\n",
    "    ydl_opts = {}\n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        info_dict = ydl.extract_info(video_url, download=False)\n",
    "        thumbnail_url = info_dict.get('thumbnail', None)\n",
    "        video_title = info_dict.get('title', 'Untitled Video')\n",
    "\n",
    "    img_path = None\n",
    "    if thumbnail_url:\n",
    "        response = requests.get(thumbnail_url)\n",
    "        if response.status_code == 200:\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            img_path = \"thumbnail.jpg\"  \n",
    "            img.save(img_path, \"JPEG\")\n",
    "        else:\n",
    "            print(\"Failed to download thumbnail.\")\n",
    "    \n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    \n",
    "    pdf.add_font('Roboto', '', 'Roboto/static/Roboto-SemiBold.ttf', uni=True)\n",
    "    pdf.set_font('Roboto', size=10)\n",
    "    \n",
    "    pdf.add_page()\n",
    "\n",
    "    if img_path:\n",
    "        page_width = pdf.w \n",
    "        img_width = 100  \n",
    "        x_centered = (page_width - img_width) / 2 \n",
    "\n",
    "        pdf.image(img_path, x=x_centered, y=10, w=img_width) \n",
    "        pdf.ln(60)  \n",
    "    #video_title = video_title.encode('utf-8').decode('utf-8')\n",
    "    #pdf.set_font('Roboto', size=14)\n",
    "    #pdf.cell(0, 10, video_title, ln=True, align='C')\n",
    "    #pdf.ln(10)  \n",
    "\n",
    "    pdf.set_font('Roboto', size=8)\n",
    "\n",
    "    checkbox_size = 4\n",
    "    x_offset = 5\n",
    "    y_offset = pdf.get_y() + 5  \n",
    "    line_height = 5\n",
    "    page_height = 297\n",
    "    margin_bottom = 15\n",
    "\n",
    "    with open(txt_filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if y_offset + line_height > page_height - margin_bottom:\n",
    "                pdf.add_page()\n",
    "                y_offset = 10  \n",
    "            \n",
    "            pdf.rect(x_offset, y_offset, checkbox_size, checkbox_size)\n",
    "            pdf.set_xy(x_offset + checkbox_size + 2, y_offset)\n",
    "            pdf.multi_cell(0, line_height, line.strip())\n",
    "            y_offset += line_height\n",
    "\n",
    "    pdf.output(pdf_filename)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
